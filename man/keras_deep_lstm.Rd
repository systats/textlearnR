% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/keras_deep_lstm.R
\name{keras_deep_lstm}
\alias{keras_deep_lstm}
\title{keras deep lstm}
\usage{
keras_deep_lstm(input_dim, embed_dim = 128, seq_len = 50,
  hidden_dims = c(128, 64, 32), bidirectional = F,
  output_fun = "softmax", output_dim = 2)
}
\arguments{
\item{input_dim}{Number of unique vocabluary/tokens}

\item{embed_dim}{Number of word vectors}

\item{seq_len}{Length of the input sequences}

\item{hidden_dims}{Number of neurons per layer as vector of integers c(256, 128, 64)}

\item{bidirectional}{default is F}

\item{output_fun}{Output activation function}

\item{output_dim}{Number of neurons of the output layer}
}
\value{
keras model
}
\description{
Word embedding + Deep (bidirectional) long short-term memory
}
\details{
Stacking lstm modules of different size.
}
